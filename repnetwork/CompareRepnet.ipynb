{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<module 'gym' from 'C:\\\\Users\\\\zenith\\\\PycharmProjects\\\\MachineLearning\\\\venv\\\\lib\\\\site-packages\\\\gym\\\\__init__.py'>\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import gym\n",
    "import numpy as np\n",
    "import statistics\n",
    "import tensorflow as tf\n",
    "import tqdm\n",
    "from actor_critic import train_step, ActorCritic, create_env\n",
    "from repnet.RepTree import TreeRL\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create the environment\n",
    "print(gym)\n",
    "env = create_env()\n",
    "\n",
    "# Set seed for experiment reproducibility\n",
    "seed = 42\n",
    "env.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Small epsilon value for stabilizing division operations\n",
    "eps = np.finfo(np.float32).eps.item()\n",
    "\n",
    "min_episodes_criterion = 100\n",
    "max_episodes = 10000\n",
    "max_steps_per_episode = 1000\n",
    "\n",
    "# Cartpole-v0 is considered solved if average reward is >= 195 over 100\n",
    "# consecutive trials\n",
    "reward_threshold = 195\n",
    "running_reward = 0\n",
    "\n",
    "# Discount factor for future rewards\n",
    "gamma = 0.99\n",
    "\n",
    "num_actions = env.action_space.n  # 2\n",
    "num_hidden_units = 128\n",
    "\n",
    "runs_to_avg = 10"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\zenith\\PycharmProjects\\MachineLearning\\actor_critic\\RunEpisodes.py\", line 153, in train_step  *\n        action_probs, values, rewards = run_episode(\n    File \"C:\\Users\\zenith\\PycharmProjects\\MachineLearning\\actor_critic\\RunEpisodes.py\", line 69, in run_episode  *\n        action_logits_t, value = model(state)\n    File \"C:\\Users\\zenith\\PycharmProjects\\MachineLearning\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\zenith\\AppData\\Local\\Temp\\__autograph_generated_files9b7l5vz.py\", line 10, in tf__call\n        x = ag__.converted_call(ag__.ld(self).common, (ag__.ld(inputs),), None, fscope)\n\n    ValueError: Exception encountered when calling layer \"actor_critic_2\" (type ActorCritic).\n    \n    in user code:\n    \n        File \"C:\\Users\\zenith\\PycharmProjects\\MachineLearning\\actor_critic\\RunEpisodes.py\", line 33, in call  *\n            x = self.common(inputs)\n        File \"C:\\Users\\zenith\\PycharmProjects\\MachineLearning\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n    \n        ValueError: tf.function only supports singleton tf.Variables created on the first call. Make sure the tf.Variable is only created once or created outside tf.function. See https://www.tensorflow.org/guide/function#creating_tfvariables for more information.\n    \n    \n    Call arguments received by layer \"actor_critic_2\" (type ActorCritic):\n      â€¢ inputs=tf.Tensor(shape=(1, 4), dtype=float32)\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[1;32mIn [5]\u001B[0m, in \u001B[0;36m<cell line: 2>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m t:\n\u001B[0;32m     10\u001B[0m     initial_state \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mconstant(env\u001B[38;5;241m.\u001B[39mreset(), dtype\u001B[38;5;241m=\u001B[39mtf\u001B[38;5;241m.\u001B[39mfloat32)\n\u001B[1;32m---> 11\u001B[0m     episode_reward \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mint\u001B[39m(\u001B[43mtrain_step\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     12\u001B[0m \u001B[43m        \u001B[49m\u001B[43minitial_state\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgamma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_steps_per_episode\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m     14\u001B[0m     episodes_reward\u001B[38;5;241m.\u001B[39mappend(episode_reward)\n\u001B[0;32m     15\u001B[0m     running_reward \u001B[38;5;241m=\u001B[39m statistics\u001B[38;5;241m.\u001B[39mmean(episodes_reward)\n",
      "File \u001B[1;32m~\\PycharmProjects\\MachineLearning\\venv\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    151\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    152\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[1;32m--> 153\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[0;32m    154\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    155\u001B[0m   \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file0i27yjs9.py:12\u001B[0m, in \u001B[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_step\u001B[1;34m(initial_state, model, optimizer, gamma, max_steps_per_episode)\u001B[0m\n\u001B[0;32m     10\u001B[0m retval_ \u001B[38;5;241m=\u001B[39m ag__\u001B[38;5;241m.\u001B[39mUndefinedReturnValue()\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m ag__\u001B[38;5;241m.\u001B[39mld(tf)\u001B[38;5;241m.\u001B[39mGradientTape() \u001B[38;5;28;01mas\u001B[39;00m tape:\n\u001B[1;32m---> 12\u001B[0m     (action_probs, values, rewards) \u001B[38;5;241m=\u001B[39m \u001B[43mag__\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconverted_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43mag__\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mld\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrun_episode\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mag__\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mld\u001B[49m\u001B[43m(\u001B[49m\u001B[43minitial_state\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mag__\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mld\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mag__\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mld\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmax_steps_per_episode\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfscope\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     13\u001B[0m     returns \u001B[38;5;241m=\u001B[39m ag__\u001B[38;5;241m.\u001B[39mconverted_call(ag__\u001B[38;5;241m.\u001B[39mld(get_expected_return), (ag__\u001B[38;5;241m.\u001B[39mld(rewards), ag__\u001B[38;5;241m.\u001B[39mld(gamma)), \u001B[38;5;28;01mNone\u001B[39;00m, fscope)\n\u001B[0;32m     14\u001B[0m     (action_probs, values, returns) \u001B[38;5;241m=\u001B[39m [ag__\u001B[38;5;241m.\u001B[39mconverted_call(ag__\u001B[38;5;241m.\u001B[39mld(tf)\u001B[38;5;241m.\u001B[39mexpand_dims, (ag__\u001B[38;5;241m.\u001B[39mld(x), \u001B[38;5;241m1\u001B[39m), \u001B[38;5;28;01mNone\u001B[39;00m, fscope) \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m [ag__\u001B[38;5;241m.\u001B[39mld(action_probs), ag__\u001B[38;5;241m.\u001B[39mld(values), ag__\u001B[38;5;241m.\u001B[39mld(returns)]]\n",
      "File \u001B[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_fileb03garly.py:68\u001B[0m, in \u001B[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__run_episode\u001B[1;34m(initial_state, model, max_steps)\u001B[0m\n\u001B[0;32m     66\u001B[0m action_logits_t \u001B[38;5;241m=\u001B[39m ag__\u001B[38;5;241m.\u001B[39mUndefined(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124maction_logits_t\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     67\u001B[0m continue_ \u001B[38;5;241m=\u001B[39m ag__\u001B[38;5;241m.\u001B[39mUndefined(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcontinue_\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m---> 68\u001B[0m \u001B[43mag__\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfor_stmt\u001B[49m\u001B[43m(\u001B[49m\u001B[43mag__\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconverted_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43mag__\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mld\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtf\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrange\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mag__\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mld\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmax_steps\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfscope\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mextra_test\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloop_body\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mget_state_1\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mset_state_1\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43maction_probs\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mrewards\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mvalues\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mbreak_\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mstate\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43miterate_names\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mt\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     69\u001B[0m action_probs \u001B[38;5;241m=\u001B[39m ag__\u001B[38;5;241m.\u001B[39mconverted_call(ag__\u001B[38;5;241m.\u001B[39mld(action_probs)\u001B[38;5;241m.\u001B[39mstack, (), \u001B[38;5;28;01mNone\u001B[39;00m, fscope)\n\u001B[0;32m     70\u001B[0m values \u001B[38;5;241m=\u001B[39m ag__\u001B[38;5;241m.\u001B[39mconverted_call(ag__\u001B[38;5;241m.\u001B[39mld(values)\u001B[38;5;241m.\u001B[39mstack, (), \u001B[38;5;28;01mNone\u001B[39;00m, fscope)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_fileb03garly.py:31\u001B[0m, in \u001B[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__run_episode.<locals>.loop_body\u001B[1;34m(itr)\u001B[0m\n\u001B[0;32m     29\u001B[0m (break_,)\n\u001B[0;32m     30\u001B[0m state \u001B[38;5;241m=\u001B[39m ag__\u001B[38;5;241m.\u001B[39mconverted_call(ag__\u001B[38;5;241m.\u001B[39mld(tf)\u001B[38;5;241m.\u001B[39mexpand_dims, (ag__\u001B[38;5;241m.\u001B[39mld(state), \u001B[38;5;241m0\u001B[39m), \u001B[38;5;28;01mNone\u001B[39;00m, fscope)\n\u001B[1;32m---> 31\u001B[0m (action_logits_t, value) \u001B[38;5;241m=\u001B[39m \u001B[43mag__\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconverted_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43mag__\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mld\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mag__\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mld\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfscope\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     32\u001B[0m action \u001B[38;5;241m=\u001B[39m ag__\u001B[38;5;241m.\u001B[39mconverted_call(ag__\u001B[38;5;241m.\u001B[39mld(tf)\u001B[38;5;241m.\u001B[39mrandom\u001B[38;5;241m.\u001B[39mcategorical, (ag__\u001B[38;5;241m.\u001B[39mld(action_logits_t), \u001B[38;5;241m1\u001B[39m), \u001B[38;5;28;01mNone\u001B[39;00m, fscope)[\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m     33\u001B[0m action_probs_t \u001B[38;5;241m=\u001B[39m ag__\u001B[38;5;241m.\u001B[39mconverted_call(ag__\u001B[38;5;241m.\u001B[39mld(tf)\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39msoftmax, (ag__\u001B[38;5;241m.\u001B[39mld(action_logits_t),), \u001B[38;5;28;01mNone\u001B[39;00m, fscope)\n",
      "File \u001B[1;32m~\\PycharmProjects\\MachineLearning\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     65\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:  \u001B[38;5;66;03m# pylint: disable=broad-except\u001B[39;00m\n\u001B[0;32m     66\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[1;32m---> 67\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[0;32m     68\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m     69\u001B[0m   \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_files9b7l5vz.py:10\u001B[0m, in \u001B[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001B[1;34m(self, inputs)\u001B[0m\n\u001B[0;32m      8\u001B[0m do_return \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m      9\u001B[0m retval_ \u001B[38;5;241m=\u001B[39m ag__\u001B[38;5;241m.\u001B[39mUndefinedReturnValue()\n\u001B[1;32m---> 10\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[43mag__\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconverted_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43mag__\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mld\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcommon\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mag__\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mld\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfscope\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     12\u001B[0m     do_return \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[1;31mValueError\u001B[0m: in user code:\n\n    File \"C:\\Users\\zenith\\PycharmProjects\\MachineLearning\\actor_critic\\RunEpisodes.py\", line 153, in train_step  *\n        action_probs, values, rewards = run_episode(\n    File \"C:\\Users\\zenith\\PycharmProjects\\MachineLearning\\actor_critic\\RunEpisodes.py\", line 69, in run_episode  *\n        action_logits_t, value = model(state)\n    File \"C:\\Users\\zenith\\PycharmProjects\\MachineLearning\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\zenith\\AppData\\Local\\Temp\\__autograph_generated_files9b7l5vz.py\", line 10, in tf__call\n        x = ag__.converted_call(ag__.ld(self).common, (ag__.ld(inputs),), None, fscope)\n\n    ValueError: Exception encountered when calling layer \"actor_critic_2\" (type ActorCritic).\n    \n    in user code:\n    \n        File \"C:\\Users\\zenith\\PycharmProjects\\MachineLearning\\actor_critic\\RunEpisodes.py\", line 33, in call  *\n            x = self.common(inputs)\n        File \"C:\\Users\\zenith\\PycharmProjects\\MachineLearning\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n    \n        ValueError: tf.function only supports singleton tf.Variables created on the first call. Make sure the tf.Variable is only created once or created outside tf.function. See https://www.tensorflow.org/guide/function#creating_tfvariables for more information.\n    \n    \n    Call arguments received by layer \"actor_critic_2\" (type ActorCritic):\n      â€¢ inputs=tf.Tensor(shape=(1, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "normal_iter_avg = 0\n",
    "for _ in range(runs_to_avg):\n",
    "    episodes_reward: collections.deque = collections.deque(maxlen=min_episodes_criterion)\n",
    "    model = ActorCritic(num_actions, num_hidden_units)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "    iters = 0\n",
    "    with tqdm.trange(max_episodes) as t:\n",
    "        for i in t:\n",
    "            initial_state = tf.constant(env.reset(), dtype=tf.float32)\n",
    "            episode_reward = int(train_step(\n",
    "                initial_state, model, optimizer, gamma, max_steps_per_episode))\n",
    "\n",
    "            episodes_reward.append(episode_reward)\n",
    "            running_reward = statistics.mean(episodes_reward)\n",
    "\n",
    "            iters += 1\n",
    "\n",
    "            t.set_description(f'Episode {i}')\n",
    "            t.set_postfix(\n",
    "                episode_reward=episode_reward, running_reward=running_reward)\n",
    "\n",
    "            # Show average episode reward every 10 episodes\n",
    "            if i % 10 == 0:\n",
    "                pass  # print(f'Episode {i}: average reward: {avg_reward}')\n",
    "\n",
    "            if running_reward > reward_threshold and i >= min_episodes_criterion:\n",
    "                break\n",
    "    normal_iter_avg += iters\n",
    "normal_iter_avg /= 10\n",
    "print(f\"Average number of iterations to solve CartPole using the basic approach across 10 runs: {normal_iter_avg}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "episodes_reward: collections.deque = collections.deque(maxlen=min_episodes_criterion)\n",
    "model = ActorCritic(num_actions, num_hidden_units)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "iter_avg = 0\n",
    "\n",
    "\n",
    "trunk = TreeRL(100, lambda x: 100, model)\n",
    "\n",
    "for end in trunk.get_branch_ends():\n",
    "    end.weights = model.get_weights()\n",
    "\n",
    "for _ in range(runs_to_avg):\n",
    "    iters = 0\n",
    "    for i in range(max_episodes):\n",
    "        # print(trunk.get_branch_ends())\n",
    "        for end in trunk.get_branch_ends():\n",
    "            if end.killed:\n",
    "                continue\n",
    "\n",
    "            if end.weights is not None:\n",
    "                model.set_weights(end.weights)\n",
    "            iters += 1\n",
    "\n",
    "            initial_state = tf.constant(env.reset(), dtype=tf.float32)\n",
    "            episode_reward = int(train_step(\n",
    "                initial_state, model, optimizer, gamma, max_steps_per_episode))\n",
    "\n",
    "            # print(episode_reward)\n",
    "\n",
    "            episodes_reward.append(episode_reward)\n",
    "            running_reward = statistics.mean(episodes_reward)\n",
    "\n",
    "            end.weights = model.get_weights()\n",
    "\n",
    "\n",
    "            # Show average episode reward every 10 episodes\n",
    "            if i % 50 == 0:\n",
    "                print(trunk.main)\n",
    "                print(f'Episode {i}: reward: {episode_reward}')\n",
    "\n",
    "            if running_reward > reward_threshold and i >= min_episodes_criterion:\n",
    "                break\n",
    "\n",
    "            trunk.update_end(end, episode_reward, model.get_weights())\n",
    "    iter_avg += iters\n",
    "iter_avg /= 10\n",
    "print(f\"Average number of iterations to solve CartPole using REPNET across 10 runs: {iter_avg}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}