{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Procedural REPNET Analysis"
      ],
      "metadata": {
        "id": "zQtjZv_0PLMl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qU_NMHYPPB3u"
      },
      "outputs": [],
      "source": [
        "# Import needed modules and define epsilon\n",
        "import collections\n",
        "import random\n",
        "import gym\n",
        "import numpy as np\n",
        "import statistics\n",
        "import tensorflow as tf\n",
        "import tqdm\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import List, Tuple\n",
        "\n",
        "# Define epsilon through numpy\n",
        "eps = np.finfo(np.float32).eps.item()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the basic Actor-Critic model:\n",
        "\n"
      ],
      "metadata": {
        "id": "GfrCSyaFPnYO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ActorCritic(tf.keras.Model):\n",
        "    \"\"\"Combined actor_critic network.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            num_actions: int,\n",
        "            num_hidden_units: int):\n",
        "        \"\"\"Initialize.\"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.common = layers.Dense(num_hidden_units, activation=\"relu\")\n",
        "        self.actor = layers.Dense(num_actions)\n",
        "        self.critic = layers.Dense(1)\n",
        "\n",
        "    def call(self, inputs: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor]:\n",
        "        x = self.common(inputs)\n",
        "        return self.actor(x), self.critic(x)\n",
        "\n",
        "\n",
        "class CartPoleEnv:\n",
        "    def __init__(self):\n",
        "        self.env = gym.make(\"CartPole-v1\")\n",
        "\n",
        "    def env_step(self, action: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "        \"\"\"Returns state, reward and done flag given an action.\"\"\"\n",
        "\n",
        "        state, reward, done, _ = self.env.step(action)\n",
        "        return (state.astype(np.float32),\n",
        "                np.array(reward, np.int32),\n",
        "                np.array(done, np.int32))\n",
        "\n",
        "    def tf_env_step(self, action: tf.Tensor) -> List[tf.Tensor]:\n",
        "        return tf.numpy_function(self.env_step, [action],\n",
        "                                 [tf.float32, tf.int32, tf.int32])\n",
        "\n",
        "    def run_episode(self,\n",
        "            initial_state: tf.Tensor,\n",
        "            model: tf.keras.Model,\n",
        "            max_steps: int) -> Tuple[tf.Tensor, tf.Tensor, tf.Tensor]:\n",
        "        \"\"\"Runs a single episode to collect training data.\"\"\"\n",
        "\n",
        "        action_probs = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
        "        values = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
        "        rewards = tf.TensorArray(dtype=tf.int32, size=0, dynamic_size=True)\n",
        "\n",
        "        initial_state_shape = initial_state.shape\n",
        "        state = initial_state\n",
        "\n",
        "        for t in tf.range(max_steps):\n",
        "            # Convert state into a batched tensor (batch size = 1)\n",
        "            state = tf.expand_dims(state, 0)\n",
        "\n",
        "            # Run the model and to get action probabilities and critic value\n",
        "            action_logits_t, value = model(state)\n",
        "\n",
        "            # Sample next action from the action probability distribution\n",
        "            action = tf.random.categorical(action_logits_t, 1)[0, 0]\n",
        "            action_probs_t = tf.nn.softmax(action_logits_t)\n",
        "\n",
        "            # Store critic values\n",
        "            values = values.write(t, tf.squeeze(value))\n",
        "\n",
        "            # Store log probability of the action chosen\n",
        "            action_probs = action_probs.write(t, action_probs_t[0, action])\n",
        "\n",
        "            # Apply action to the environment to get next state and reward\n",
        "            state, reward, done = self.tf_env_step(action)\n",
        "            state.set_shape(initial_state_shape)\n",
        "\n",
        "            # Store reward\n",
        "            rewards = rewards.write(t, reward)\n",
        "\n",
        "            if tf.cast(done, tf.bool):\n",
        "                break\n",
        "\n",
        "        action_probs = action_probs.stack()\n",
        "        values = values.stack()\n",
        "        rewards = rewards.stack()\n",
        "\n",
        "        return action_probs, values, rewards\n",
        "\n",
        "    def get_expected_return(\n",
        "            self,\n",
        "            rewards: tf.Tensor,\n",
        "            gamma: float,\n",
        "            standardize: bool = True) -> tf.Tensor:\n",
        "        \"\"\"Compute expected returns per timestep.\"\"\"\n",
        "\n",
        "        n = tf.shape(rewards)[0]\n",
        "        returns = tf.TensorArray(dtype=tf.float32, size=n)\n",
        "\n",
        "        # Start from the end of `rewards` and accumulate reward sums\n",
        "        # into the `returns` array\n",
        "        rewards = tf.cast(rewards[::-1], dtype=tf.float32)\n",
        "        discounted_sum = tf.constant(0.0)\n",
        "        discounted_sum_shape = discounted_sum.shape\n",
        "        for i in tf.range(n):\n",
        "            reward = rewards[i]\n",
        "            discounted_sum = reward + gamma * discounted_sum\n",
        "            discounted_sum.set_shape(discounted_sum_shape)\n",
        "            returns = returns.write(i, discounted_sum)\n",
        "        returns = returns.stack()[::-1]\n",
        "\n",
        "        if standardize:\n",
        "            returns = ((returns - tf.math.reduce_mean(returns)) /\n",
        "                       (tf.math.reduce_std(returns) + eps))\n",
        "\n",
        "        return returns\n",
        "\n",
        "    def compute_loss(\n",
        "            self,\n",
        "            action_probs: tf.Tensor,\n",
        "            values: tf.Tensor,\n",
        "            returns: tf.Tensor) -> tf.Tensor:\n",
        "        \"\"\"Computes the combined actor_critic loss.\"\"\"\n",
        "        huber_loss = tf.keras.losses.Huber(reduction=tf.keras.losses.Reduction.SUM)\n",
        "\n",
        "        advantage = returns - values\n",
        "\n",
        "        action_log_probs = tf.math.log(action_probs)\n",
        "        actor_loss = -tf.math.reduce_sum(action_log_probs * advantage)\n",
        "\n",
        "        critic_loss = huber_loss(values, returns)\n",
        "\n",
        "        return actor_loss + critic_loss\n",
        "\n",
        "    @tf.function\n",
        "    def train_step(\n",
        "            self,\n",
        "            initial_state: tf.Tensor,\n",
        "            model: tf.keras.Model,\n",
        "            optimizer: tf.keras.optimizers.Optimizer,\n",
        "            gamma: float,\n",
        "            max_steps_per_episode: int) -> tf.Tensor:\n",
        "        \"\"\"Runs a model training step.\"\"\"\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            # Run the model for one episode to collect training data\n",
        "            action_probs, values, rewards = self.run_episode(\n",
        "                initial_state, model, max_steps_per_episode)\n",
        "\n",
        "            # Calculate expected returns\n",
        "            returns = self.get_expected_return(rewards, gamma)\n",
        "\n",
        "            # Convert training data to appropriate TF tensor shapes\n",
        "            action_probs, values, returns = [\n",
        "                tf.expand_dims(x, 1) for x in [action_probs, values, returns]]\n",
        "\n",
        "            # Calculating loss values to update our network\n",
        "            loss = self.compute_loss(action_probs, values, returns)\n",
        "\n",
        "        # Compute the gradients from the loss\n",
        "        grads = tape.gradient(loss, model.trainable_variables)\n",
        "\n",
        "        # Apply the gradients to the model's parameters\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "        episode_reward = tf.math.reduce_sum(rewards)\n",
        "\n",
        "        return episode_reward"
      ],
      "metadata": {
        "id": "Isxn4hsWPigH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the REPNET architecture:"
      ],
      "metadata": {
        "id": "Hpq6n6uXQWl6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MinBranch:\n",
        "    def __init__(self, max_kill_iters, threshold_func, performance=0, weights=None, generation=0):\n",
        "        self.max_kill_iters = max_kill_iters\n",
        "        self.reset_iters = 0\n",
        "\n",
        "        self.threshold_func = threshold_func\n",
        "        self.performance = performance\n",
        "        self.weights = weights\n",
        "        self.child_branches = []\n",
        "        self.killed = False\n",
        "\n",
        "        self.generation = generation\n",
        "\n",
        "    def check_kill(self):\n",
        "        if self.reset_iters > self.max_kill_iters:\n",
        "            self.killed = True\n",
        "\n",
        "    def get_branch_num_ends(self):\n",
        "        if len(self.child_branches) == 0:\n",
        "            return 1\n",
        "\n",
        "        branch_end_sum = 0\n",
        "        for child in self.child_branches:\n",
        "            branch_end_sum += child.get_branch_num_ends()\n",
        "        return branch_end_sum\n",
        "\n",
        "    def get_branch_ends(self):\n",
        "        if len(self.child_branches) == 0:\n",
        "            return [self]\n",
        "\n",
        "        branch_ends = []\n",
        "        for child in self.child_branches:\n",
        "            branch_ends.append(child.get_branch_ends())\n",
        "        return np.array(branch_ends).flatten()\n",
        "\n",
        "    def update(self, new_performance, new_weights):\n",
        "        self.check_kill()\n",
        "        if self.killed:\n",
        "            # del self\n",
        "            return True\n",
        "\n",
        "        if new_performance - self.performance > self.threshold_func(self.performance):\n",
        "            self.child_branches.append(MinBranch(self.max_kill_iters,\n",
        "                                                 self.threshold_func,\n",
        "                                                 performance=new_performance,\n",
        "                                                 weights=new_weights,\n",
        "                                                 generation=self.generation + 1\n",
        "                                                 ))\n",
        "            self.reset_iters = 0\n",
        "        self.reset_iters += 1\n",
        "        return False\n",
        "\n",
        "    def __str__(self):\n",
        "        ret_str = \"\"\n",
        "\n",
        "        # Check and see if main branch, if so specify\n",
        "        if self.generation == 1:\n",
        "            ret_str += \"trunk: \"\n",
        "        else:\n",
        "            ret_str += \"branch: \"\n",
        "\n",
        "        if not self.killed:\n",
        "            ret_str += f\"Generation {self.generation},\" \\\n",
        "                       f\"Performance: {self.performance}, Iterations since reset: {self.reset_iters}\\n\"\n",
        "        for branch in self.child_branches:\n",
        "            for i in range(self.generation):\n",
        "                ret_str += \"    \"\n",
        "            ret_str += \"↳\" + branch.__str__()\n",
        "\n",
        "        return ret_str\n",
        "\n",
        "class TreeRL:\n",
        "    def __init__(self, max_kill_iters, threshold_func, model_baseline):\n",
        "        self.main = MinBranch(max_kill_iters, threshold_func)\n",
        "        self.model_baseline = model_baseline\n",
        "\n",
        "        self.best_weights = None\n",
        "        self.best_performance = 0\n",
        "\n",
        "    def get_num_branch_ends(self):\n",
        "        return self.main.get_branch_num_ends()\n",
        "\n",
        "    def get_branch_ends(self):\n",
        "        return self.main.get_branch_ends()\n",
        "\n",
        "    def update_end(self, end, performance, weights):\n",
        "        if performance > self.best_performance:\n",
        "            self.best_weights = weights\n",
        "            self.best_performance = performance\n",
        "        return end.update(performance, weights)"
      ],
      "metadata": {
        "id": "H8DOVUByQYUG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the training functions:"
      ],
      "metadata": {
        "id": "rEDTCfsxPynF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training functions\n",
        "\n",
        "def train_repnet(threshold_func=lambda x: 100, kill_time=100, gamma=0.99, max_episodes=10000, learning_rate=0.01,\n",
        "                 reward_threshold=195, seed=None):\n",
        "    \"\"\"\n",
        "    Trains a basic REPNET\n",
        "    :param seed: the randomizng seed to train on\n",
        "    :param threshold_func: the input function to deterimine when to create a new branch\n",
        "    :param kill_time: the maximum number of episodes from the last time of reproduction\n",
        "    that a new branch can exist for until it dies\n",
        "    :param gamma: the discount factor for future rewards\n",
        "    :param max_episodes: the maximum amount of episodes that can be trained until the training session exits (hard stop)\n",
        "    :param learning_rate: The learning rate of the Adam optimizer\n",
        "    :param reward_threshold: The minimum running reward needed to consider the model trained\n",
        "    :return: A list of the running reward every episode\n",
        "    \"\"\"\n",
        "    cartpole = CartPoleEnv()\n",
        "\n",
        "    if seed is None:\n",
        "        seed = random.randrange(0, 100)\n",
        "    # cartpole.env.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    min_episodes_criterion = 140\n",
        "    max_steps_per_episode = 1000\n",
        "\n",
        "    # Cartpole-v0 is considered solved if average reward is >= 195 over 100\n",
        "    # consecutive trials\n",
        "    running_reward = 0\n",
        "\n",
        "    # Keep last episodes reward\n",
        "    episodes_reward: collections.deque = collections.deque(maxlen=min_episodes_criterion)\n",
        "\n",
        "    num_actions = cartpole.env.action_space.n  # 2\n",
        "    num_hidden_units = 128\n",
        "\n",
        "    model = ActorCritic(num_actions, num_hidden_units)\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "    trunk = TreeRL(kill_time, threshold_func, model)\n",
        "\n",
        "    for end in trunk.get_branch_ends():\n",
        "        end.weights = model.get_weights()\n",
        "\n",
        "    performance_list = []\n",
        "\n",
        "    iters = 0\n",
        "    for i in range(max_episodes):\n",
        "        # print(trunk.get_branch_ends())\n",
        "        for end in trunk.get_branch_ends():\n",
        "            if end.killed:\n",
        "                continue\n",
        "\n",
        "            if end.weights is not None:\n",
        "                model.set_weights(end.weights)\n",
        "            iters += 1\n",
        "\n",
        "            initial_state = tf.constant(cartpole.env.reset(), dtype=tf.float32)\n",
        "            episode_reward = int(cartpole.train_step(\n",
        "                initial_state, model, optimizer, gamma, max_steps_per_episode))\n",
        "\n",
        "            episodes_reward.append(episode_reward)\n",
        "            running_reward = statistics.mean(episodes_reward)\n",
        "\n",
        "            performance_list.append(running_reward)\n",
        "\n",
        "            end.weights = model.get_weights()\n",
        "\n",
        "            if running_reward > reward_threshold:\n",
        "                return performance_list\n",
        "\n",
        "            trunk.update_end(end, episode_reward, model.get_weights())\n",
        "    if performance_list[-1] < reward_threshold:\n",
        "        return train_repnet()\n",
        "    print(len(performance_list))\n",
        "    return performance_list\n",
        "\n",
        "\n",
        "def train_distributed_repnet(threshold_func=lambda x: 100, kill_time=100, gamma=0.99, max_episodes=10000,\n",
        "                             learning_rate=0.01, reward_threshold=195, seed=None):\n",
        "    \"\"\"\n",
        "    Trains REPNET based on the multiprocessing distribution principle\n",
        "    :param seed: the randomizing seed to train on\n",
        "    :param threshold_func: the input function to deterimine when to create a new branch\n",
        "    :param kill_time: the maximum number of episodes from the last time of reproduction\n",
        "    that a new branch can exist for until it dies\n",
        "    :param gamma: the discount factor for future rewards\n",
        "    :param max_episodes: the maximum amount of episodes that can be trained until the training session exits (hard stop)\n",
        "    :param learning_rate: The learning rate of the Adam optimizer\n",
        "    :param reward_threshold: The minimum running reward needed to consider the model trained\n",
        "    :return: A list of the running reward every episode\n",
        "    \"\"\"\n",
        "    cartpole = CartPoleEnv()\n",
        "\n",
        "    if seed is None:\n",
        "        seed = random.randrange(0, 100)\n",
        "    # cartpole.env.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    min_episodes_criterion = 140\n",
        "    max_steps_per_episode = 1000\n",
        "\n",
        "    # Cartpole-v0 is considered solved if average reward is >= 195 over 100\n",
        "    # consecutive trials\n",
        "    running_reward = 0\n",
        "\n",
        "    # Keep last episodes reward\n",
        "    episodes_reward: collections.deque = collections.deque(maxlen=min_episodes_criterion)\n",
        "\n",
        "    num_actions = cartpole.env.action_space.n  # 2\n",
        "    num_hidden_units = 128\n",
        "\n",
        "    model = ActorCritic(num_actions, num_hidden_units)\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "    trunk = TreeRL(kill_time, threshold_func, model)\n",
        "\n",
        "    for end in trunk.get_branch_ends():\n",
        "        end.weights = model.get_weights()\n",
        "\n",
        "    performance_list = []\n",
        "\n",
        "    iters = 0\n",
        "    for i in range(max_episodes):\n",
        "        performance_list.append(running_reward)\n",
        "        for end in trunk.get_branch_ends():\n",
        "            if end.killed:\n",
        "                continue\n",
        "\n",
        "            if end.weights is not None:\n",
        "                model.set_weights(end.weights)\n",
        "            iters += 1\n",
        "\n",
        "            initial_state = tf.constant(cartpole.env.reset(), dtype=tf.float32)\n",
        "            episode_reward = int(cartpole.train_step(\n",
        "                initial_state, model, optimizer, gamma, max_steps_per_episode))\n",
        "\n",
        "            episodes_reward.append(episode_reward)\n",
        "            running_reward = statistics.mean(episodes_reward)\n",
        "\n",
        "            end.weights = model.get_weights()\n",
        "\n",
        "            if running_reward > reward_threshold:\n",
        "                return performance_list\n",
        "\n",
        "            trunk.update_end(end, episode_reward, model.get_weights())\n",
        "    if performance_list[-1] < reward_threshold:\n",
        "        return train_distributed_repnet()\n",
        "    print(len(performance_list))\n",
        "    return performance_list\n",
        "\n",
        "\n",
        "def train_rl(gamma=0.99, max_episodes=10000, learning_rate=0.01, reward_threshold=195, seed=None):\n",
        "    \"\"\"\n",
        "    Trains a normal actor-critic network\n",
        "    :param seed: The randomizing seed to train on\n",
        "    :param gamma: the discount factor for future rewards\n",
        "    :param max_episodes: the maximum amount of episodes that can be trained until the training session exits (hard stop)\n",
        "    :param learning_rate: The learning rate of the Adam optimizer\n",
        "    :param reward_threshold: The minimum running reward needed to consider the model trained\n",
        "    :return: A list of the running reward every episode\n",
        "    \"\"\"\n",
        "    cartpole = CartPoleEnv()\n",
        "\n",
        "    if seed is None:\n",
        "        seed = random.randrange(0, 100)\n",
        "    # cartpole.env.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    # Small epsilon value for stabilizing division operations\n",
        "    eps = np.finfo(np.float32).eps.item()\n",
        "\n",
        "    min_episodes_criterion = 100\n",
        "    max_steps_per_episode = 1000\n",
        "\n",
        "    # Cartpole-v0 is considered solved if average reward is >= 195 over 100\n",
        "    # consecutive trials\n",
        "    running_reward = 0\n",
        "\n",
        "    # Keep last episodes reward\n",
        "    episodes_reward: collections.deque = collections.deque(maxlen=min_episodes_criterion)\n",
        "\n",
        "    num_actions = cartpole.env.action_space.n  # 2\n",
        "    num_hidden_units = 128\n",
        "\n",
        "    model = ActorCritic(num_actions, num_hidden_units)\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "    performance_list = []\n",
        "\n",
        "    iters = 0\n",
        "\n",
        "    with tqdm.trange(max_episodes) as t:\n",
        "        for i in t:\n",
        "            initial_state = tf.constant(cartpole.env.reset(), dtype=tf.float32)\n",
        "            episode_reward = int(cartpole.train_step(\n",
        "                initial_state, model, optimizer, gamma, max_steps_per_episode))\n",
        "\n",
        "            episodes_reward.append(episode_reward)\n",
        "            running_reward = statistics.mean(episodes_reward)\n",
        "\n",
        "            iters += 1\n",
        "\n",
        "            performance_list.append(running_reward)\n",
        "\n",
        "            t.set_description(f'Episode {i}')\n",
        "            t.set_postfix(\n",
        "                episode_reward=episode_reward, running_reward=running_reward)\n",
        "\n",
        "            # Show average episode reward every 10 episodes\n",
        "            if i % 10 == 0:\n",
        "                pass  # print(f'Episode {i}: average reward: {avg_reward}')\n",
        "\n",
        "            if running_reward > reward_threshold and i >= min_episodes_criterion:\n",
        "                break\n",
        "    if performance_list[-1] < reward_threshold:\n",
        "        return train_rl()\n",
        "    return performance_list"
      ],
      "metadata": {
        "id": "1CooZ0aCPZE8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analyze network performance:"
      ],
      "metadata": {
        "id": "2dMENvpuP_XX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of each network to train and then average\n",
        "num_networks_trained = 1\n",
        "\n",
        "# Lists that will store the running reward data across episodes\n",
        "repnet_performances = []\n",
        "dist_repnet_performances = []\n",
        "rl_performances = []\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # The loop to train each of the three types of networks num_networks_trained amount of times\n",
        "    for i in range(num_networks_trained+1):\n",
        "        if i % 10 == 0 and i > 0:\n",
        "          print(f\"{100*i/num_networks_trained}% done!\")\n",
        "          dist_repnet_data = [len(x) for x in dist_repnet_performances]\n",
        "\n",
        "\n",
        "        # Different hyperparameters can be set using kwargs\n",
        "        # repnet_perform = train_repnet()\n",
        "        dist_repnet_perform = train_distributed_repnet(threshold_func=lambda x: 15, kill_time=75)\n",
        "        rl_perform = train_rl()\n",
        "\n",
        "        # Not recommended to plot a large number of networks at once\n",
        "        # if i == 1:\n",
        "        #   # Only use the labels the first time\n",
        "        #   plt.plot(repnet_perform, color=\"magenta\", label=\"REPNET\")\n",
        "        #   plt.plot(dist_repnet_perform, color=\"black\", label=\"Distributed REPNET\")\n",
        "        #   plt.plot(rl_perform, color=\"teal\", label=\"Normal RL\")\n",
        "        # else:\n",
        "        #   plt.plot(repnet_perform, color=\"magenta\")\n",
        "        #   plt.plot(dist_repnet_perform, color=\"black\")\n",
        "        #   plt.plot(rl_perform, color=\"teal\")\n",
        "\n",
        "        # rl_performances.append(rl_perform)\n",
        "        # repnet_performances.append(repnet_perform)\n",
        "        dist_repnet_performances.append(dist_repnet_perform)\n",
        "\n",
        "    # Get the number of episodes for each training instance each network required to achieve the threshold (195)\n",
        "    # repnet_data = [len(x) for x in repnet_performances]\n",
        "    dist_repnet_data = [len(x) for x in dist_repnet_performances]\n",
        "    # rl_data = [len(x) for x in rl_performances]\n",
        "\n",
        "    # Print the mean and standard deviation of each network across num_networks_trained number of each network\n",
        "    # print(f\"REPNET average # of weight updates: {np.mean(repnet_data)}, standard dev: {np.std(repnet_data)}\")\n",
        "    print(f\"Distributed REPNET average # of weight updates: {np.mean(dist_repnet_data)}, standard dev: {np.std(dist_repnet_data)}\")\n",
        "    # print(f\"RL average # of weight updates {np.mean(rl_data)}, standard dev: {np.std(rl_data)}\")\n",
        "\n",
        "    # plt.title(\"REPNET versus normal actor-critic RL on the CartPole env:\")\n",
        "    # # plt.title(\"REPNET and distributed variant versus normal actor-critic RL on the CartPole env:\")\n",
        "    # plt.xlabel(\"Running reward\")\n",
        "    # plt.ylabel(\"Cumulative training episodes\")\n",
        "    # plt.legend()\n",
        "    # plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zGkLgq1P4j_",
        "outputId": "29c5a79c-5e83-4271-f628-5f496afa5eff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10.0% done!\n",
            "Distributed REPNET average # of weight updates: 167.0, standard dev: 34.295772334210525\n",
            "20.0% done!\n",
            "Distributed REPNET average # of weight updates: 166.25, standard dev: 35.87321424126921\n",
            "30.0% done!\n",
            "Distributed REPNET average # of weight updates: 170.66666666666666, standard dev: 37.568899667440654\n",
            "40.0% done!\n",
            "Distributed REPNET average # of weight updates: 176.225, standard dev: 37.35738715435008\n",
            "50.0% done!\n",
            "Distributed REPNET average # of weight updates: 171.2, standard dev: 42.61689805699143\n",
            "60.0% done!\n",
            "Distributed REPNET average # of weight updates: 174.93333333333334, standard dev: 40.93200323571873\n",
            "70.0% done!\n",
            "Distributed REPNET average # of weight updates: 176.68571428571428, standard dev: 41.64562843020788\n",
            "80.0% done!\n",
            "Distributed REPNET average # of weight updates: 178.6375, standard dev: 42.65918533856454\n",
            "90.0% done!\n",
            "Distributed REPNET average # of weight updates: 176.95555555555555, standard dev: 41.255548074332744\n",
            "Distributed REPNET average # of weight updates: 180.04, standard dev: 41.237342300395646\n"
          ]
        }
      ]
    }
  ]
}