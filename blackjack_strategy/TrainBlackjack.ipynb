{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Train a neural network to optimally play blackjack.\n",
    "### Author: T Lindauer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "import collections\n",
    "import statistics\n",
    "from typing import Any\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "from BlackjackEnv import BlackjackEnv"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define first model used"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "outputs": [],
   "source": [
    "modelOne = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(40,)),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='softmax')\n",
    "])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define second model used"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "outputs": [],
   "source": [
    "modelTwo = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(40,)),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='softmax')\n",
    "])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "def reward(hand_score: int) -> float:\n",
    "    return min(hand_score / 21, 0 if hand_score > 21 else 1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [
    "def gen_sparse_matrix(num: int):\n",
    "    return [0] * (num - 1) + [1] + [0] * (40 - num)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Training loop"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "outputs": [],
   "source": [
    "def run_episode(env: BlackjackEnv, initial_state_0: tf.Tensor, initial_state_1: tf.Tensor):\n",
    "\n",
    "    initial_state_shape = initial_state_0.shape\n",
    "    state_0 = initial_state_0\n",
    "    state_1 = initial_state_1\n",
    "\n",
    "    action_probs_0 = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "    values_0 = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "    rewards_0 = tf.TensorArray(dtype=tf.int32, size=0, dynamic_size=True)\n",
    "\n",
    "    action_probs_1 = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "    values_1 = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "    rewards_1 = tf.TensorArray(dtype=tf.int32, size=0, dynamic_size=True)\n",
    "\n",
    "    for t in range(0, 3):\n",
    "        # state_0 = tf.expand_dims(state_0, -1)\n",
    "        # state_1 = tf.expand_dims(state_1, -1)\n",
    "\n",
    "        state_0 = state_0[np.newaxis, :]\n",
    "        state_1 = state_1[np.newaxis, :]\n",
    "\n",
    "        print(state_0.shape)\n",
    "        print(state_1.shape)\n",
    "\n",
    "        action_logits_t_0, value_0 = modelOne(state_0, 1)\n",
    "        action_logits_t_1, value_1 = modelTwo(state_1, 1)\n",
    "\n",
    "\n",
    "        values_0 = values_0.write(t, tf.squeeze(value_0))\n",
    "        values_1 = values_1.write(t, tf.squeeze(value_1))\n",
    "\n",
    "        action_probs_t_0 = tf.nn.softmax(action_logits_t_0)\n",
    "        action_probs_t_1 = tf.nn.softmax(action_logits_t_1)\n",
    "\n",
    "        action_0 = tf.random.categorical(action_logits_t_0, 1)[0, 0]\n",
    "        action_1 = tf.random.categorical(action_logits_t_1, 1)[0, 0]\n",
    "\n",
    "        action_probs_0 = action_probs_0.write(t, action_probs_t_0[0, action_0])\n",
    "        action_probs_1 = action_probs_1.write(t, action_probs_t_1[0, action_1])\n",
    "\n",
    "        state_0.set_shape(initial_state_shape)\n",
    "        state_1.set_shape(initial_state_shape)\n",
    "\n",
    "        translated_action_0 = round(action_0)\n",
    "        translated_action_1 = round(action_1)\n",
    "        if translated_action_0:\n",
    "            env.add_card(0)\n",
    "        if translated_action_1:\n",
    "            env.add_card(1)\n",
    "\n",
    "        state_0 = gen_sparse_matrix(env.hands[0])\n",
    "        state_1 = gen_sparse_matrix(env.hands[1])\n",
    "        # state_0 = env.hands[0] / 40\n",
    "        # state_1 = env.hands[1] / 40\n",
    "\n",
    "        rewards_0 = rewards_0.write(t, reward(env.hands[0]))\n",
    "        rewards_1 = rewards_1.write(t, reward(env.hands[1]))\n",
    "\n",
    "    action_probs_0 = action_probs_0.stack()\n",
    "    action_probs_1 = action_probs_1.stack()\n",
    "\n",
    "    values_0 = values_0.stack()\n",
    "    values_1 = values_1.stack()\n",
    "\n",
    "    rewards_0 = rewards_0.stack()\n",
    "    rewards_1 = rewards_1.stack()\n",
    "\n",
    "    return action_probs_0, action_probs_1, values_0, values_1, rewards_0, rewards_1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "outputs": [],
   "source": [
    "eps = np.finfo(np.float32).eps.item()\n",
    "\n",
    "\n",
    "def get_expected_return(\n",
    "        rewards: tf.Tensor,\n",
    "        gamma: float,\n",
    "        standardize: bool = True) -> tf.Tensor:\n",
    "    \"\"\"Compute expected returns per timestep.\"\"\"\n",
    "\n",
    "    n = tf.shape(rewards)[0]\n",
    "    returns = tf.TensorArray(dtype=tf.float32, size=n)\n",
    "\n",
    "    # Start from the end of `rewards` and accumulate reward sums\n",
    "    # into the `returns` array\n",
    "    rewards = tf.cast(rewards[::-1], dtype=tf.float32)\n",
    "    discounted_sum = tf.constant(0.0)\n",
    "    discounted_sum_shape = discounted_sum.shape\n",
    "    for i in tf.range(n):\n",
    "        reward = rewards[i]\n",
    "        discounted_sum = reward + gamma * discounted_sum\n",
    "        discounted_sum.set_shape(discounted_sum_shape)\n",
    "        returns = returns.write(i, discounted_sum)\n",
    "    returns = returns.stack()[::-1]\n",
    "\n",
    "    if standardize:\n",
    "        returns = ((returns - tf.math.reduce_mean(returns)) /\n",
    "                   (tf.math.reduce_std(returns) + eps))\n",
    "\n",
    "    return returns"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "outputs": [],
   "source": [
    "def compute_loss(\n",
    "        action_probs: tf.Tensor,\n",
    "        values: tf.Tensor,\n",
    "        returns: tf.Tensor) -> tf.Tensor:\n",
    "    \"\"\"Computes the combined actor_critic loss.\"\"\"\n",
    "    huber_loss = tf.keras.losses.Huber(reduction=tf.keras.losses.Reduction.SUM)\n",
    "\n",
    "    advantage = returns - values\n",
    "\n",
    "    action_log_probs = tf.math.log(action_probs)\n",
    "    actor_loss = -tf.math.reduce_sum(action_log_probs * advantage)\n",
    "\n",
    "    critic_loss = huber_loss(values, returns)\n",
    "\n",
    "    return actor_loss + critic_loss"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(env: BlackjackEnv, initial_state_0: tf.Tensor, initial_state_1: tf.Tensor, optimizer_0: tf.keras.optimizers.Optimizer,\n",
    "               optimizer_1: tf.keras.optimizers.Optimizer, gamma: float) -> tuple[Any, Any]:\n",
    "    with tf.GradientTape() as tape:\n",
    "        action_probs_0, action_probs_1, values_0, values_1, rewards_0, rewards_1 = run_episode(env, initial_state_0, initial_state_1)\n",
    "\n",
    "        returns_0 = get_expected_return(rewards_0, gamma)\n",
    "        returns_1 = get_expected_return(rewards_1, gamma)\n",
    "\n",
    "        action_probs_0, values_0, returns_0 = [tf.expand_dims(x, 1) for x in [action_probs_0, values_0, returns_0]]\n",
    "        action_probs_1, values_1, returns_1 = [tf.expand_dims(x, 1) for x in [action_probs_1, values_1, returns_1]]\n",
    "\n",
    "        loss_0 = compute_loss(action_probs_0, values_0, returns_0)\n",
    "        loss_1 = compute_loss(action_probs_1, values_1, returns_1)\n",
    "\n",
    "    grads_0 = tape.gradient(loss_0, modelOne.trainable_variables)\n",
    "    grads_1 = tape.gradient(loss_1, modelTwo.trainable_variables)\n",
    "\n",
    "    optimizer_0.apply_gradients(zip(grads_0, modelOne.trainable_variables))\n",
    "    optimizer_1.apply_gradients(zip(grads_1, modelTwo.trainable_variables))\n",
    "\n",
    "    episode_reward_0 = tf.math.reduce_sum(rewards_0)\n",
    "    episode_reward_1 = tf.math.reduce_sum(rewards_1)\n",
    "\n",
    "    return episode_reward_0, episode_reward_1\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "outputs": [],
   "source": [
    "n_training_iters = 10_000"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "outputs": [],
   "source": [
    "optimizer_0 = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "optimizer_1 = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "gamma = 0.99"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "outputs": [],
   "source": [
    "def train():\n",
    "    performance_list_0 = []\n",
    "    performance_list_1 = []\n",
    "    episodes_reward_0: collections.deque = collections.deque(maxlen=5)\n",
    "    episodes_reward_1: collections.deque = collections.deque(maxlen=5)\n",
    "\n",
    "    for i in range(n_training_iters):\n",
    "        if i / n_training_iters * 100 == 10:\n",
    "            print(f\"Percent completion: {i / n_training_iters * 100}\")\n",
    "\n",
    "        env = BlackjackEnv(2)\n",
    "\n",
    "        # initial_state_0 = tf.constant(tf.expand_dims(gen_sparse_matrix(env.hands[0]), 0), dtype=tf.int32)\n",
    "        # initial_state_1 = tf.constant(tf.expand_dims(gen_sparse_matrix(env.hands[1]), 0), dtype=tf.int32)\n",
    "        initial_state_0 = tf.constant(gen_sparse_matrix(env.hands[0]), dtype=tf.float32)\n",
    "        initial_state_1 = tf.constant(gen_sparse_matrix(env.hands[1]), dtype=tf.float32)\n",
    "\n",
    "        episode_reward_0, episode_reward_1 = train_step(env, initial_state_0, initial_state_1, optimizer_0, optimizer_1, gamma)\n",
    "\n",
    "        episodes_reward_0.append(int(episode_reward_0))\n",
    "        episodes_reward_1.append(int(episode_reward_1))\n",
    "\n",
    "        running_reward_0 = statistics.mean(episode_reward_0)\n",
    "        running_reward_1 = statistics.mean(episode_reward_1)\n",
    "\n",
    "        performance_list_0.append(running_reward_0)\n",
    "        performance_list_1.append(running_reward_1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 40)\n",
      "(1, 40)\n"
     ]
    },
    {
     "ename": "OperatorNotAllowedInGraphError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\zenith\\AppData\\Local\\Temp\\ipykernel_57388\\3042118400.py\", line 5, in train_step  *\n        action_probs_0, action_probs_1, values_0, values_1, rewards_0, rewards_1 = run_episode(env, initial_state_0, initial_state_1)\n    File \"C:\\Users\\zenith\\AppData\\Local\\Temp\\ipykernel_57388\\2852600450.py\", line 25, in run_episode  *\n        action_logits_t_0, value_0 = modelOne(state_0, 1)\n\n    OperatorNotAllowedInGraphError: Iterating over a symbolic `tf.Tensor` is not allowed: AutoGraph did convert this function. This might indicate you are trying to use an unsupported feature.\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mOperatorNotAllowedInGraphError\u001B[0m            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [236], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn [209], line 18\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m()\u001B[0m\n\u001B[0;32m     15\u001B[0m initial_state_0 \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mconstant(gen_sparse_matrix(env\u001B[38;5;241m.\u001B[39mhands[\u001B[38;5;241m0\u001B[39m]), dtype\u001B[38;5;241m=\u001B[39mtf\u001B[38;5;241m.\u001B[39mfloat32)\n\u001B[0;32m     16\u001B[0m initial_state_1 \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mconstant(gen_sparse_matrix(env\u001B[38;5;241m.\u001B[39mhands[\u001B[38;5;241m1\u001B[39m]), dtype\u001B[38;5;241m=\u001B[39mtf\u001B[38;5;241m.\u001B[39mfloat32)\n\u001B[1;32m---> 18\u001B[0m episode_reward_0, episode_reward_1 \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43menv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minitial_state_0\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minitial_state_1\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer_0\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer_1\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgamma\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     20\u001B[0m episodes_reward_0\u001B[38;5;241m.\u001B[39mappend(\u001B[38;5;28mint\u001B[39m(episode_reward_0))\n\u001B[0;32m     21\u001B[0m episodes_reward_1\u001B[38;5;241m.\u001B[39mappend(\u001B[38;5;28mint\u001B[39m(episode_reward_1))\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    151\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    152\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[1;32m--> 153\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[0;32m    154\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    155\u001B[0m   \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:1233\u001B[0m, in \u001B[0;36mfunc_graph_from_py_func.<locals>.autograph_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m   1231\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:  \u001B[38;5;66;03m# pylint:disable=broad-except\u001B[39;00m\n\u001B[0;32m   1232\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(e, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mag_error_metadata\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m-> 1233\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mag_error_metadata\u001B[38;5;241m.\u001B[39mto_exception(e)\n\u001B[0;32m   1234\u001B[0m   \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1235\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
      "\u001B[1;31mOperatorNotAllowedInGraphError\u001B[0m: in user code:\n\n    File \"C:\\Users\\zenith\\AppData\\Local\\Temp\\ipykernel_57388\\3042118400.py\", line 5, in train_step  *\n        action_probs_0, action_probs_1, values_0, values_1, rewards_0, rewards_1 = run_episode(env, initial_state_0, initial_state_1)\n    File \"C:\\Users\\zenith\\AppData\\Local\\Temp\\ipykernel_57388\\2852600450.py\", line 25, in run_episode  *\n        action_logits_t_0, value_0 = modelOne(state_0, 1)\n\n    OperatorNotAllowedInGraphError: Iterating over a symbolic `tf.Tensor` is not allowed: AutoGraph did convert this function. This might indicate you are trying to use an unsupported feature.\n"
     ]
    }
   ],
   "source": [
    "train()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
